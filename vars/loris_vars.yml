---

# SERVICE-WIDE VARS
# -------------------------------------------------------------
#
# assumes dedicated varnish for ids and hx loris
#####


# assumes inventory_hostname is public ip
# assumes ok to get the first ec2 with tag_service loris if list of 'loris'
hx_varnish_host: "{% for host in hostvars %}\
            {% if hostvars[host].ec2_tag_service == 'hxvarnish' %}\
                {{ hostvars[host].ec2_private_ip_address }}\
            {% endif %}\
        {% endfor %}"
hx_varnish_port: '80'

# how to access external ids image server
# if configuring a vagrant cluster, ids is a mock
ids_host: '{{ (ec2_tag_cluster == "vagrant") | ternary("ids.vm", "ids.lib.harvard.edu") }}'
# this is to config varnish, who does not talk ssl
ids_port: '80'

# ids host, as exposed by dedicated varnish
# implies that there is always a dedicated varnish for ids; not able to set the
# same varnish instance for both hx and ids...
ids_varnish_host: "{% for host in hostvars %}\
            {% if hostvars[host].ec2_tag_service == 'idsvarnish' %}\
                {{ hostvars[host].ec2_private_ip_address }}\
            {% endif %}\
        {% endfor %}"
ids_varnish_port: '80'


# LORIS VARS
# -------------------------------------------------------------

# these mimic loris default for setup.py
# changes here don't actually change the actual install vars...
service_name: "loris"
service_user: '{{ service_name }}'
service_group: '{{ service_name }}'

# check in cloudformation for the device name and uncomment if extra-ebs
#ebs_device: /dev/xvdf
ebs_mountpoint: /opt/hx

rootdir: "{{ ebs_mountpoint }}"
service_rootdir: "{{ rootdir }}/{{ service_name }}"
service_venv_dir: "{{ service_rootdir }}/venv"
# though logs go to stdout, loris requires this config
service_log_dir: "{{ service_rootdir }}/log"
# uncomment, otherwise default is debug: # 'DEBUG'|'INFO'|'WARNING'|'ERROR'|'CRITICAL'
# service_log_level: "DEBUG"

service_install_dir: '{{ service_rootdir }}/{{ service_name }}'
service_static_dir: '{{ service_rootdir }}/www'
service_config_dir: '{{ service_rootdir }}/conf'
service_config_path: '{{ service_config_dir }}/{{ service_name }}.conf'
service_bin_dir: '{{ service_rootdir }}/bin'
service_data_dir: '{{ service_rootdir }}/data'
service_tmp_dir: '{{ service_data_dir }}/tmp'
service_cache_dir: '{{ service_data_dir }}/cache'
service_images_dir: '{{ service_data_dir }}/images'

# to have loris use s3resolver (hxloris.s3resolver.S3Resolver), instead of the
# default FSResorver, define `loris_s3resolver` with the s3bucket_map as
# explained in hxloris readme: https://github.com/nmaekawa/hxloris
loris_use_s3resolver: true
default_bucket_map: '{"s3bucket": {"bucket": "my-s3-bucket", "key_prefix": "loris/images"}}'
loris_s3resolver:
  resolver_git_pip_install: 'git+https://github.com/nmaekawa/hxloris.git@master'
  resolver_impl: 'hxloris.s3resolver.S3Resolver'
  s3bucket_map: "{{ lookup('env', 'LORIS_S3BUCKET_MAP') | default(default_bucket_map, true) }}"

# it is also possible to configure a step to pre-populate  the image source
# dir: the playbooks can copy image source files from a local tar.gz file, or
# do an s3 sync with an s3 bucket.
#loris_download_source_from_s3: "{{ lookup('env', 'LORIS_DOWNLOAD_SOURCE_FROM_S3') }}"
#loris_download_source_from_tar_gz: "{{ lookup('env', 'LORIS_DOWNLOAD_SOURCE_FROM_TAR_GZ') }}"

# the same goes for the loris cache processed images
#loris_download_cache_from_s3: "{{ lookup('env', 'LORIS_DOWNLOAD_CACHE_FROM_S3') }}"
#loris_download_cache_from_tar_gz: "{{ lookup('env', 'LORIS_DOWNLOAD_CACHE_FROM_TAR_GZ') }}"

# how gunicorn exposes access to loris-hx
# hack for vagrant, didn't figure why 2 ip addr
hx_host: "{% for host in hostvars %}\
            {% if hostvars[host].ec2_tag_service == 'loris' %}\
                    {{ hostvars[host].ec2_private_ip_address }}\
            {% endif %}\
        {% endfor %}"
hx_port: '9090'
service_gunicorn_port: '{{ hx_port }}'
service_gunicorn_host: '{{ hx_host }}'
gunicorn_timeout_seconds: 300  # workers timeout - used in nginx cfg


# easier to create dirs with this dict
service_dirs_to_create:
    - '{{ service_rootdir }}'
    - '{{ service_bin_dir }}'
    - '{{ service_log_dir }}'
    - '{{ service_static_dir }}'
    - '{{ service_config_dir }}'
    - '{{ service_data_dir }}'
    - '{{ service_tmp_dir }}'
    - '{{ service_cache_dir }}'
    - '{{ service_images_dir }}'


# for nmaekawa.apt
apt_required_packages_loris:
    - 'python3-pip'
    - 'python3-dev'
    - 'python3-setuptools'
    - 'libjpeg-turbo8-dev'
    - 'libfreetype6-dev'
    - 'zlib1g-dev'
    - 'liblcms2-dev'
    - 'liblcms2-utils'
    - 'libtiff5-dev'
    - 'libwebp-dev'
    - 'supervisor'


# for Stouts.users
users_enabled: yes
users_to_install:
    - 'nmaekawa'
    - 'lduarte'
    - '{{ service_user }}'


# VARNISH VARS
# -------------------------------------------------------------

#varnish_enabled_services:
#    - varnish
#    - varnishncsa

varnish_instances:
    hx:
        frontend:  # how clients talk to varnish
            host: '{{ hx_varnish_host }}'
            port: '{{ hx_varnish_port }}'
        backend:
            host: '{{ hx_host }}'
            host_header: '{{ webserver_dns }}'
            port: '{{ hx_port }}'
    ids:
        frontend:
            host: '{{ ids_varnish_host }}'
            port: '{{ ids_varnish_port }}'
        backend:
            host: '{{ ids_host }}'
            host_header: '{{ ids_host }}'
            port: '{{ ids_port }}'


# PROXY VARS
# -------------------------------------------------------------

# to be overwritten when aws cluster; used when forward proxying to libraries
#proxy_dns_resolver: '8.8.8.8' # google resolver

# for nginx, talk to image servers via varnish
ids_image_host: '{{ varnish_instances.ids.frontend.host }}'
ids_image_port: '{{ varnish_instances.ids.frontend.port }}'
hx_image_host: '{{ varnish_instances.hx.frontend.host }}'
hx_image_port: '{{ varnish_instances.hx.frontend.port }}'

# att: service_static_dir and the actual static_dir for nginx config might be
# different if proxy is to be in separate instance than the service(loris)
# - you can change this in the playbook, in **vars for include_role**
static_dir: '{{ service_static_dir }}'
cert_name: 'images'  # this name must match pattern for ssl cert names
# special case for vagrant
vagrant_webserver_dns: 'images.vm'

# beware that order of location matter in nginx matching rules!
# for now, webdav servers should always have location '/'
# for now, do not set webdav and static location in same nginx server
# beware that nginx role concats location.root + location.prefix to create
#   the directory where static files should exist
nginx_servers:
  - nginx_template_path: roles/nginx/templates/proxy_server.j2
    service_server_name: '{{ service_name }}_proxy'
    nginx_nonssl_port: 80
    nginx_ssl_port: 443
    locations:
      - name: 'ids_proxy'
        prefix: '~ ^/ids/'
        root: '{{ service_static_dir }}'
        getonly: 'yes'
        gunicorn_timeout_seconds: '{{ gunicorn_timeout_seconds }}'
        service_gunicorn_host: '{{ ids_image_host }}'
        service_gunicorn_port: '{{ ids_image_port }}'
      - name: 'hx_proxy'
        prefix: '~ ^/iiif/'
        root: '{{ service_static_dir }}'
        getonly: 'yes'
        add_rewrites: 'yes'
        gunicorn_timeout_seconds: '{{ gunicorn_timeout_seconds }}'
        service_gunicorn_host: '{{ hx_image_host }}'
        service_gunicorn_port: '{{ hx_image_port }}'

